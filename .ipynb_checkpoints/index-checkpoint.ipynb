{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "present-supervision",
   "metadata": {},
   "source": [
    "# Ingesting Data Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-senate",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-spider",
   "metadata": {},
   "source": [
    "In this lesson, we'll work on connecting to our redshift database, and then taking data from a CSV file in S3 and importing it into a table in redshift.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-chamber",
   "metadata": {},
   "source": [
    "### Setting up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-import",
   "metadata": {},
   "source": [
    "If you haven't already, create a redshift cluster and attach the necessary policies.  \n",
    "1. Create the cluster\n",
    "\n",
    "\n",
    "Remember that to create the redshift cluster, it's best to go to the redshift v1 dashbooard [located here](https://console.aws.amazon.com/redshift) and then click on clusters, followed by `Launch Cluster`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-battle",
   "metadata": {},
   "source": [
    "2. Set the appropriate details\n",
    "\n",
    "<img src=\"./redshift-config.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-locking",
   "metadata": {},
   "source": [
    "3. And under the `additional config` section, we can associate the correct security group, and associate the appropriate role that we created in previous lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-jumping",
   "metadata": {},
   "source": [
    "> <img src=\"./additional-config.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-cornell",
   "metadata": {},
   "source": [
    "Then after the review page, and creating the cluster, we can wait for the cluster to setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-touch",
   "metadata": {},
   "source": [
    "> <img src=\"./creating-cluster.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-helen",
   "metadata": {},
   "source": [
    "When our dashboard switches from `creating` to `available`, we can properly connect to our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-breathing",
   "metadata": {},
   "source": [
    "### Adding data to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-moldova",
   "metadata": {},
   "source": [
    "Remember that we'll want to have the ability to take data from an S3 bucket, and import it into redshift.  Ultimately, we'll get that data from an RDS instance, but for now we can skip that step and simply upload our data to an S3 bucket, to then import it into our RDS instance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-hybrid",
   "metadata": {},
   "source": [
    "So go to the S3 resource, and create a new bucket with something like the pattern:\n",
    "\n",
    "> `your-initials-foursquare-data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-viewer",
   "metadata": {},
   "source": [
    "And then find the csv csv files located in the data folder of this lab and upload them to the s3 bucket.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-meaning",
   "metadata": {},
   "source": [
    "> <img src=\"./categories-venues.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-circuit",
   "metadata": {},
   "source": [
    "Now that our data is uploaded, and our redshift cluster is created, it's time to import our data from S3 and into our redshift database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-relaxation",
   "metadata": {},
   "source": [
    "### Working in Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-indian",
   "metadata": {},
   "source": [
    "We can begin working in redshift by connecting to our database.  Import the `psycopg2` library, and create a connection and cursor into the redshift database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "straight-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "commercial-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-density",
   "metadata": {},
   "source": [
    "> Below, check that the connection information is appropriate for the redshift cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "helpful-forth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<connection object at 0x110ad1580; dsn: 'user=awsuser password=xxx dbname=dev host=redshift-cluster-1.cdpgnoufdsdf.us-east-1.redshift.amazonaws.com port=5439', closed: 0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn\n",
    "# <connection object at 0x110ad1580; \n",
    "# dsn: 'user=awsuser \n",
    "# password=xxx dbname=dev\n",
    "# host=redshift-cluster-1.cdpgnoufdsdf.us-east-1.redshift.amazonaws.com port=5439', \n",
    "# closed: 0>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-premises",
   "metadata": {},
   "source": [
    "Next, let's get ready to import in some data, beginning with the categories information.  To do this we should first create the categories table.  Create the table with the correct columns based on the csv file.  The text columns should be of type `Varchar` allowing for `200` characters.\n",
    "\n",
    ">  If needed, take a look at the [create table examples](https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_examples.html) or the [datatypes available in redshift ](https://docs.aws.amazon.com/redshift/latest/dg/c_Supported_data_types.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "flying-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_categories_command = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "specific-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(create_categories_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "relative-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-function",
   "metadata": {},
   "source": [
    "Then we can check that the columns of our tables by querying `PG_TABLE_DEF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "electronic-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"SELECT  * FROM  PG_TABLE_DEF WHERE schemaname = 'public';\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exotic-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.fetchall()\n",
    "# [('public', 'categories', 'id', 'integer', 'az64', False, 0, True),\n",
    "#  ('public',  'categories',  'name',  'character varying(200)',  'lzo',  False,  0,  False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-scout",
   "metadata": {},
   "source": [
    "So here, we can see that there is a `categories`, `id` column of type integer.  And that the second column is a `categories`, `name` column of type `character varying(200)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-october",
   "metadata": {},
   "source": [
    "Now let's try to use redshift to upload in the data from our `categories.csv` file into our table.  Let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "primary-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "universal-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-compatibility",
   "metadata": {},
   "source": [
    "Next, confirm that we imported the data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "piano-basic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 'Pizza'), (45, 'Italian')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('SELECT * FROM categories LIMIT 2;')\n",
    "\n",
    "cursor.fetchall()\n",
    "# [(44, 'Pizza'), (45, 'Italian')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-intelligence",
   "metadata": {},
   "source": [
    "### One last table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-chair",
   "metadata": {},
   "source": [
    "Ok, now let's work on importing information from the `venues.csv` file.  \n",
    "\n",
    "\n",
    "> The `price` and `rating` columns both cannot be too large, so set the datatype of both to small int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "liquid-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "stock-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_venues_command = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "optimum-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(create_venues_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eligible-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-mailing",
   "metadata": {},
   "source": [
    "Then let's again check the columns in our database and we should see some more added in there. \n",
    "\n",
    "> This time we'll select columns just from the `venues` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "hired-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"SELECT * FROM PG_TABLE_DEF WHERE schemaname = 'public';\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "senior-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-excitement",
   "metadata": {},
   "source": [
    "Next let's import in some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "adopted-venice",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError_",
     "evalue": "Load into table 'venues' failed.  Check 'stl_load_errors' system table for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError_\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-379a163eaaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mInternalError_\u001b[0m: Load into table 'venues' failed.  Check 'stl_load_errors' system table for details.\n"
     ]
    }
   ],
   "source": [
    "qry = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-grill",
   "metadata": {},
   "source": [
    "Uh oh, we should have gotten an error.  Recreate the connection to the database, and then we can begin to debug the error with a call to the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "built-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"select query, substring(filename,22,25) as filename,line_number as line, \n",
    "substring(colname,0,12) as column, type, position as pos, substring(raw_line,0,50) as line_text,\n",
    "substring(raw_field_value,0,15) as field_text, \n",
    "substring(err_reason,0,45) as reason\n",
    "from stl_load_errors \n",
    "order by query desc\n",
    "limit 1;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "circular-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "settled-credit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1090,\n",
       "  'uare/venues.csv',\n",
       "  6,\n",
       "  'rating',\n",
       "  'int2      ',\n",
       "  50,\n",
       "  '57,5b2932a0f5e9d70039787cf2,Los Tacos Al Pastor,1',\n",
       "  '8.0',\n",
       "  \"Invalid digit, Value '.', Pos 1, Type: Short\")]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()\n",
    "\n",
    "# (1090,\n",
    "#   'uare/venues.csv',\n",
    "#   6,\n",
    "#   'rating',\n",
    "#   'int2      ',\n",
    "#   50,\n",
    "#   '57,5b2932a0f5e9d70039787cf2,Los Tacos Al Pastor,1',\n",
    "#   '8.0',\n",
    "#   \"Invalid digit, Value '.', Pos 1, Type: Short\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-woman",
   "metadata": {},
   "source": [
    "So the above is telling us that there was an issue with the inserting data into the rating column for the entry of Lost Tacos Al Pastor.  We may want to look at the value directly, but we can see under `Invalid digit, value '.'` that it looks like there was a `.` when there should not have been.  Notice that the rating is of type integer, and perhaps it should not be.  \n",
    "\n",
    "Drop the venues table, recreate it, updating the datatype of the `rating` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "sophisticated-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"\"\"  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "otherwise-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "protected-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_venues_command = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "patent-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(create_venues_command)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-input",
   "metadata": {},
   "source": [
    "Now let's try to copy over the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "solved-relevance",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError_",
     "evalue": "Load into table 'venues' failed.  Check 'stl_load_errors' system table for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError_\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-379a163eaaa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mInternalError_\u001b[0m: Load into table 'venues' failed.  Check 'stl_load_errors' system table for details.\n"
     ]
    }
   ],
   "source": [
    "qry = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-truck",
   "metadata": {},
   "source": [
    "Sorry, another error.  Let's again see if we can find the issue.  We'll need to check `stl_load_errors` again and see if it's any different than the last one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-intro",
   "metadata": {},
   "source": [
    "> Remember we'll again have to recreate the connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fiscal-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"select query, substring(filename,22,25) as filename,line_number as line, \n",
    "substring(colname,0,12) as column, type, position as pos, substring(raw_line,0,50) as line_text,\n",
    "substring(raw_field_value,0,15) as field_text, \n",
    "substring(err_reason,0,45) as reason\n",
    "from stl_load_errors \n",
    "order by query desc\n",
    "limit 1;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "detected-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "optimum-bibliography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1170,\n",
       "  'uare/venues.csv',\n",
       "  109,\n",
       "  'price',\n",
       "  'int2      ',\n",
       "  39,\n",
       "  '160,4fc26c1be4b0d516256e64e3,\"taqueria, Mexican &',\n",
       "  ' Mexican & Sal',\n",
       "  \"Invalid digit, Value 'M', Pos 1, Type: Short\")]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-parameter",
   "metadata": {},
   "source": [
    "If we look at the error, we see something like the following:\n",
    "```json\n",
    "'160,4fc26c1be4b0d516256e64e3,\"taqueria, Mexican &',\n",
    "  ' Mexican & Sal',\n",
    "  \"Invalid digit, Value 'M', Pos 1, Type: Short\")]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-therapist",
   "metadata": {},
   "source": [
    "So for some reeason, `Mexican and Sal` is being placed in the price column.  Let's open up the csv file and look at the offending row to take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-football",
   "metadata": {},
   "source": [
    "<img src=\"./tacqueria.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-partition",
   "metadata": {},
   "source": [
    "Looking at row 109, just the row our error message said there was a problem at, we can see that `\"tacqueria, Mexican\"` has a comma in it.  This is likely throwing off the csv parsing.  Perhaps there's a way to ignore commas when we are copying over our CSV file into our table.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-limitation",
   "metadata": {},
   "source": [
    "Take a look at the following [stackoverflow post](https://stackoverflow.com/questions/42720342/escaping-delimiter-in-amazon-redshift-copy-command) about how to specify what is in quotation marks and then give it another shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "unsigned-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "planned-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-swift",
   "metadata": {},
   "source": [
    "After a couple more attempts, hopefully we can get it working.  If so we should not see an error message, and perhaps can will be able to retrieve our first two records from venues with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "phantom-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('SELECT * FROM venues LIMIT 2;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "apart-registration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(53,\n",
       "  '',\n",
       "  'Los Tacos Al Pastor',\n",
       "  1,\n",
       "  None,\n",
       "  None,\n",
       "  '',\n",
       "  datetime.datetime(2020, 12, 22, 18, 37, 39, 384899)),\n",
       " (54,\n",
       "  '1234',\n",
       "  'Grimaldis',\n",
       "  2,\n",
       "  2.0,\n",
       "  3,\n",
       "  'grimaldis.com',\n",
       "  datetime.datetime(2020, 12, 22, 18, 37, 39, 395055))]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-family",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-context",
   "metadata": {},
   "source": [
    "In this lesson, we saw how we can create, connect to, and import data into our redshift cluster.  We also practiced working through error messages in redshift.  As we saw, debugging redshift is a bit indirect as the error messages are stored on the `stl_load_errors` table.  We also saw that it's important for us to think about what can go wrong with our data, such as using the correct datatype, and accounting for extra commas in our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
